ollama:
  # mountPath: "/mnt/ollama"
  gpu:
    # -- Enable GPU integration
    enabled: true
    
    # -- GPU type: 'nvidia' or 'amd'
    type: 'nvidia'
    
    # -- Specify the number of GPU to 1
    number: 1
   
  # -- List of models to pull at container startup
  # models: 
  #   - mistral
  #   - llama3.1:8b

# persistentVolume:
#   enabled: true
#   accessModes: ["ReadWriteMany"]
#   size: 1000Gi
#   storageClass: "nfs-provisioner"
#   volumeMode: Filesystem
#   ReclaimPolicy: Retain
persistentVolume:
  enabled: true
#  accessModes: ["ReadWriteMany"]
#   size: 1000Gi
#   storageClass: "nfs-csi"
#   volumeMode: Filesystem
  existingClaim: ollama-claim


# volumeMounts:
#   - name: ollama-pvc
#     mountPath: "/mnt"
volumes:
  - name: ollama-claim
    persistentVolumeClaim:
      claimName: ollama-claim
extraEnv: 
  - name: OLLAMA_NUM_PARALLEL
    value: "1"